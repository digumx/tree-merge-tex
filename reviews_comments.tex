\section{Reviews and Comments Regd. Same}

\subsection{Review 1}

 As a major concern, the approach is proposed based on the existing  CEGAR
 framework, and the technical contributions are rather  incremental. The paper
 basically introduces an order over the merges  of the neurons and thus it can
 exploit the order for an effective  refinement. However, the order, which
 relies on the concept of  semantic closeness metric, is rather simple and
 heuristic. A naive  approach to the estimation of this metric is given in
 Section 3.1,  however, the quality of this metric may depend heavily on
 sampling and  therefore, it may be very unstable. Moreover, this part is not
 experimentally evaluated.   Another concern is regarding the experiment,
 especially the usefulness  of the approach. One application should be neural
 network  verification, as stated in the introduction and as performed in
 Section 4.3. However, there is no comparison with any baseline  approach. There
 could be many possible baselines, including the  existing neural network
 verification approaches for the original  network, or existing abstraction and
 refinement approach. In the  current shape, the value of the proposed approach
 is not clearly  demonstrated.

 \dmcmt{What can we do about the above, beyond better experiments?}

  In Section 4.1, I don’t get the statement “there are certain  ‘critical‘
  classes for which a false negative classification is far  more dangerous than
  a false positive one.” \todo{Explain this better}

  Also, the experiment design of Section 4.1 is not very clear. First,  the
  evaluation metrics, such as Reduction, Steps are not rigorously  defined.
  Second, I cannot map very well the content in this part with  the previous
  contents in Section 3.3.1. In particular, it’s not clear  which approach is
  the one introduced in Section 3.3.1, and which  approach is the one that the
  authors want to sell. The experimental  results show that random, namely, the
  baseline approach, already gives  a very good performance.
  In Section 4.1, it seems the authors want to claim that the strategy  of
  finding gamma is not very important, but in my view, the approach  becomes
  less interesting because it doesn’t matter which technique it  adopts.

  \dmcmt{I have tried to rework 3.3.1 and 4.1 to address some of the above}

  \subsection{Review 2}
  
 I have several concerns or questions as follows:
  1. It would be helpful to improve the writing of the introduction,  making it
     more organized so that readers can better understand the  basic idea of
     this work more systematically. \todo{Re-work intro}
  2. The explanation and proof of the building of the tree lack some  details. A
     more detailed explanation of the PairwiseMax algorithm  would be helpful:
 1) How does the framework use a greedy algorithm to initialize the  tree of
    merges?  2) For the building process of the tree of merges, does the
    framework  calculate the semantic closeness of each pair of neutrons and
    keep the  combination that has the largest semantic closeness?
 3) What does the PairwiseMax algorithm do to calculate the semantic  closeness?
    How to calculate the PairwiseMax of m1 and m2 since they  are two neutrons?
  3. The framework cuts until the tree of merges reaches the desired  quality.
     How to measure the quality of the merges?
     \todo{Address above concerns}
  4. Tale 1 provides false positive values. However, more metrics might  be
     helpful to show the performance and quality of the framework.
     \dmcmt{How? Can we do something here?}

\subsection{Review 3}
     
 The proposed heuristic is simple (which is a positive) and makes a lot  of
 sense. However, the negative of the paper is its experimental  evaluation: the
 author do not evaluate the runtime performance of  their algorithm against any
 other algorithm. While they use  alpha-beta-Crown for some sub-tasks (eg.
 generating PGD samples), so  they could have at least compared against that.
 \dmcmt{This is fair, but i don't think we will get much improvement in perf
     over alpha beta crown. Integrating neural sat is another option. What
 should we do? For compression, maybe we can use guy katz as a baseline?}
 Further, the  evaluation is only on a few MNIST networks from VNNComp; there is
 no  rationale provided why these benchmarks were used and others were  omitted.
 \dmcmt{Another fair point, but I don't know if we will have time to expand
 this to others?}
 The paper is, for most parts, well-written, and quite easy to follow.  However,
 the plots could have been better: none of the plots label the  x- and y-axis,
 and neither are the labels mentioned in the text,  making it difficult to
 understand the experimental results. \todo{Fix this}

