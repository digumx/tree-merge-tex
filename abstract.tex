
\begin{abstract}
Deep Neural Networks (DNNs), today, are being trained and trusted for
performing fairly complex tasks, even in safety-critical applications such as
automated driving, medical diagnostics, and air-traffic control. Proving
trustworthiness, typically using formal verification or formal explainability,
is a difficult task though. The biggest challenge in this comes from the size
of the DNNs -- real-world networks are trained for performance, and
compromising size for accuracy is therefore a natural thing to do. To address
this issue, several abstraction methods have been proposed. This includes
techniques that perform an abstract analysis of the network, and those that
abstract or compress the network itself.  We study the latter, in which there
are two different approaches that have been taken: \emph{syntactic}
abstraction, with formal soundness guarantees, and \emph{semantic} abstraction
where the soundness guarantees are usually much weaker, and empirical. In this
paper, we propose to combine the semantic and syntactic approaches into a
single framework, to get the best of both worlds. This allows us to guide the
abstraction using global semantic information while still providing concrete
soundness guarantees based on syntactic constraints. Our experiments on
standard neural network benchmarks shows that this can be effective for both
verification as well as compression.
\end{abstract}

\iffalse

\begin{abstract}
Neural Networks are increasingly being used in several safety critical
applications, including medical imaging, self driving cars and aircraft
collision detection. To gain trust on the networks being deployed in safety
critical domains and reduce the risk of safety violations, several techniques
based on formal analysis has been proposed, including formal verification and
formal explainability. However, the scalability of these techniques is highly
sensitive to the size of the networks involved, limiting the applicability of
these techniques to real-world networks. To handle these scalability issues,
structural abstraction based on syntactic techniques have been proposed, and
while these provide formal soundness guarantees, they do not take into account
the semantic behavior of the network. On the other hand, neural network
compression and semantic abstraction \dmcmt{Jan, few lines in intro}
techniques take into account this information, but the soundness guarantees they
provide are generally much weaker, and empirical. We propose to combine both
the semantic and syntactic approaches into a single framework to try to obtain
the best of both worlds, guiding our abstraction using global semantic
information while still providing concrete soundness guarantees based on
syntactic constraints. We do this by constructing a partial order of best
possible merges using global semantic information, and represent it using a
tree-like data structure. Then we follow this tree to perform syntactic merge
and example guided \sncmt{Some other wording?} refinement operations. This
allows us to obtain abstract networks that is guided by global semantic
information while providing concrete soundness guarantees. We demonstrate the
effectiveness of these abstractions on standard neural network benchmarks via
experiments.
\end{abstract}

\fi

