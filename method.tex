\section{Methodology}
\label{s:semantic-closeness}


\dmcmt{Im not leading with the lang of partial order here, instead defining a
    tree directly. This, I feel is easier to motivate and explain. What we have
    really is more than just a partial order of merge operations - it is in fact
    a tree. It is can be motivated by saying that it lets us define merge groups
    in a way that is in some sense optimal. It is also easier to explain
    precisely what the tree is by going over it's construction in an algo.
    The tree then can be used to define / talk about the partial order in the
intro, or the intro can itself be modified to refer to a tree instead. }

\todo{Add refs to later sections}
 
In this work we aim to utilize information from the semantic behavior of \cnc to
better control and guide the refinement process producing higher quality \abs
while retaining concrete soundness. 

In particular, we define a \textit{semantic
closeness metric} that captures how close the semantic-behavior of two 
neurons are. We utilise this semantic closeness metric to arrange the merge
operations into a tree where lower quality merges involving semantically far
neurons appear higher and can be refined with higher priority. 

Using this tree we build a framework \dmcmt{Is this okay?} where refinement can
be done by making cuts of this tree. We show that the merge groups coming from
such cuts have the property that any two neurons within the same group are 
semantically closer than any two neurons from two different groups. Thus, via
this tree-based refinement process, we are able to partition the neurons in \cnc
into merge groups\dmcmt{Is it clear what a merge group is?} in a manner that is
optimal with respect to the semantic information.
This allows us to avoid restoring large number of single neurons(see Section
\ref{s:nn-sam}) and lets us retain merge operations of higher quality. Thus, we
produce an \abs of the desired quality with a much smaller size. \todo{Ref later
sections}

While the choice of which nodes to leave merged together is guided by the cuts
of the tree, the weights attached to the abstract nodes are still chosen
by following the procedures from Section \ref{s:nn-sam} and \cite{cegar-nn}.
Thus, the concrete soundness guarantees still hold for the \abs obtained.

In the following sections we describe a general framework for such a unified
syntactic and semantic refinement process, describing each component in detail.

\dmcmt{This will be the general structure of the rest of this section. The
existing material can be re-organised into these headers.}

\subsection{Semantic Closeness Factor}

\todo{If we are able to get the characterestic based exps run, we can talk
    about that as well.
}

To guide the semantic abstraction process, we define a \textit{semantic
closeness metric} $\cls$: $\cls(\nr{i_1}{l}, \nr{i_2}{l})$ is function that
takes two neurons $\nr{i_1}{l}$ and $\nr{i_2}{l}$ in
the same layer $l$, and returns a real number that
captures how close the behaviors of $\nr{i_1}{l}$ and $\nr{i_2}{l}$ are from a
semantic point of view. The number returned by $\cls$ should be smaller for
neurons whose semantic behavior is closer. Intuitively, this metric would
characterise the semantic behavior of the
neurons in layer $l$ relative to each other, and prioritise certain merges over
others. 

Depending on the application, the precise definition of this metric may be
chosen in various ways. We note that our framework is agnostic to the particular
choice of semantic metric, and a concrete soundness guarantee holds for any such
choice. Inspired by \cite{deep-abstract}, in we chose as the
semantic closeness metric how close the functions computed by the two neurons
are: $||\nrf{i_1}{l} - \nrf{i_2}{l}||$. 

However, since $\nrf{i_1}{l}$ and $\nrf{i_2}{l}$ are functions, computing
$||\nrf{i_1}{l} - \nrf{i_2}{l}||$ precisely is not feasible.
Therefore, we estimate it using a sample set of inputs $X$: $||\ob{i_1}{l}($X$)
- \ob{i_2}{l}($X$)||_2$. This $X$ may be chosen in many ways, for example, it
may be a set of randomly chosen input values satisfying $P$ when
abstracting with respect to a query $(P, \mcnc, Q)$
(Section \ref{s:exp-mnist-rob}), or a dataset when attempting to find a
compression with no new false negatives (Section \ref{s:exp-mnist-comp}).

\subsection{Tree of Merges}
\label{s:tree}

We use $\cls$ to create a tree structure to prioritise merges wherein leaf nodes
represent the original neurons, and 
non-leaf nodes represent merge operations. The construction of the tree 
follows a bottom-up approach, starting from individual neurons, 
greedily performing the merge operation involving the most similar groups of
neurons, and delaying the merging of dissimilar ones. \todo{Refer to guarantees}

Note that $\cls$ only provides us
with a similarity measure for pairs of neurons. For this tree construction
algorithm to work we must extend this to a semantic closeness
metric between groups of neurons
$\{\nr{i_1}{l}, {\cdots}, \nr{i_p}{l}\}$ and $\{\nr{j_1}{l}, {\cdots},
\nr{j_q}{l}\}$. To do this we take the pairwise maximum \todo{Motivate choice of
maximum via section on optimum}: 

\begin{equation*}
\begin{aligned}
    \max_{k_1 \in \{1, {\cdots}, p\},
    k_2 \in \{1, {\cdots}, q\}} \cls( \nr{i_{k_1}}{l}, \nr{j_{k_2}}{l} )
\end{aligned}
\end{equation*}

\dmcmt{Is the above notation necessary, or can we just say pairwise maximum and
refer to algo?}

This process is detailed in Algorithm \ref{a:build-tree}.

\begin{algorithm}[H]
\caption{Building the Tree}
\label{a:build-tree}
\begin{algorithmic}[1]

    \Require Neurons $\{\nr{i_1}{l}, {\cdots}, \nr{i_r}{l}\}$ with same class,
    Closeness metric $\cls$

    
    \State Initialize a Binary Tree $T$ with leaves as
        $\{m_1, {\cdots}, m_r\}$ corresponding to $\{ \nr{i_1}{l}, {\cdots},
        \nr{i_r}{l} \}$
    %\State Initialize map $M(m_k) = \{ \nr{i_k}{l} \}$ taking tree nodes to sets
    %    of neurons in corresponding merge group
    \State Initialize $Q=\{m_1, {\cdots}, m_r\}$ as the set of leaf nodes.

    \Function{PairwiseMax}{$m, m'$}
        
            \State Let $L, L'$ be leaves of sub-tree rooted at $m, m'$

            \Return $\max_{\nr{i_{k}}{l} \in S, \nr{i_{k'}}{l} \in S'} 
                \cls( \nr{i_{k}}{l}, \nr{i_{k'}}{l} )$

    \EndFunction

    \While{$|Q|>1$}
        \State $m_{j_1}, m_{j_2} = \arg\min_{\substack{m, m' \in Q}} 
            \text{PairwiseMax}(m, m')$
        \State Add new node $m_{j_3}$ to $T$ 
        \State Make $m_{j_1}, m_{j_2}$ children on $m_{j_3}$
        \State Remove $m_{j_1}, m_{j_2}$ from $Q$ and add $m_{j_3}$ to $Q$.
    \EndWhile

    \Ensure Tree of merges $T$
\end{algorithmic}
\end{algorithm}

For instance, considering the middle layer in Figure
\ref{fig:Original_Net_Property}, $\nr{0}{1}$ and $\nr{1}{1}$ are semantically
closest. Thus, in the tree, we merge these two first to get the node $m_4$.
Then, the two semantically closest pairs are given by $\nr{2}{1}$ and
$\nr{3}{1}$, so they are merged to $m_5$. Finally, $m_4$ and $m_5$ gets merged
to $m_6$. This gives us the tree seen in the top half of Figure
\ref{fig:Order_of_merging}.\dmcmt{Is it okay to refer to same figure? Can it be
confusing potentially?}

%In figure \ref{Figure: Order Of Merging}, if we give the list of input vectors $[[1], [2], [3]]$ to the observation function $o_{(i, j)}$ for $(n_{(1, 1, Inc)})$, then we obtain a list of output vectors $[[1.5], [3], [4.5]]$. We utilize the output of the observation function as a measure to merge neurons. For instance, considering three neurons $n_{(w, i)}$, $n_{(w, j)}$, and $n_{(w, k)}$ with 
%observation vectors $o_{(w, i)}$, $o_{(w, j)}$, and 
%$o_{(w, k)}$, the merging sequence adheres to the following conditions: 
%
%If $||o_{(w, i)} - o_{(w, j)}||_{2} \leq ||o_{(w, i)} - 
%o_{(w, k)}||_{2}$ and $||o_{(w, i)} - o_{(w, j)}||_2 \leq ||o_{(w, j)} - 
%o_{(w, k)}||_{2}$, then $n_{(w, i)}$ and $n_{(w, j)}$ are initially merged into a 
%representative neuron $\alpha$, followed by the merging of $\alpha$ and $n_{(w, k)}$. 
%Here $||o_{(w, i)} - o_{(w, j)}||_{2}$ computes the ``$\textit{Euclidean 
%Distance}$" between the observation vectors $o_{(w, i)}$ and  $o_{(w, j)}$.
%
%
%In Figure \ref{Figure: Order Of Merging}, for the first layer, the initial 
%merge would involve the neuron $n_{(1, 0, Inc)}$ with $n_{(1, 1, Inc)}$, 
%forming $\textit{Merge 1}$, then we would combine $n_{(1, 2, Inc)}$ and 
%$n_{(1, 3, Inc)}$, forming $\textit{Merge 2}$. Finally we combine 
%$\textit{Merge 1}$ and $\textit{Merge 2}$ into the root node. 

As we progress up the tree, the coarseness introduced by the 
merge operation increases. 
This is due to the increasing difference in the semantic behavior of nodes
involved. \todo{Connect with optimality}

In our example, the merge operation
$m_4$ involves $\{\nr{0}{1}, \nr{1}{1}\}$, while $m_6$ involves $\{\nr{0}{1},
\nr{1}{1}, \nr{2}{1}, \nr{3}{1}\}$. Since the semantic behavior of $\nr{0}{1}$
and $\nr{1}{1}$ are closer than $\nr{0}{1}$, $\nr{1}{1}$, $\nr{2}{1}$,
$\nr{3}{1}$, operation $m_4$ represents a less coarse abstraction than $m_6$.

For our choice of $\cls$, (Section \ref{s:semantic-closeness}), Algorithm
\ref{a:build-tree} reduces to \hcluster, allowing us to leverage existing
efficient implementations \cite{scipy-hcluster-linkage}. Nonetheless, the
general algorithm presented here will work for any choice of $\cls$.

\subsection{Tree-cuts and Refinement}
\label{s:refinement}

\input{our_tree_struct.tex}

In our abstraction refinement loop, we start with the fully merged network.
Then, whenever we get a \gencex $\vct{\beta}$ (Section \ref{s:qual})
\dmcmt{Do we discuss where we get this beta from? Or is it okay to just talk abt
it in the experiments section, with some brief things in intro to this
section?}, we wish to refine the network, that is, we wish to chose which
neurons should remain merged. Intuitively, this choice should be guided
two factors: respecting the semantic behavior of each node, and attempting to
eliminate $\vct{\beta}$.

The tree produced in the previous Section \ref{s:tree} captures the semantic
behavior, and we use it to guide the refinement process as follows:
Any cut of the tree produces a set of trees. Then,  
the groups of neurons that we chose to keep merged correspond to the leaf nodes
of the  these trees. Therefore finding a refinement is reduces to finding a cuts
in the tree.

To attempt to eliminate $\vct{\beta}$, we identify a \textit{culprit neuron}
$\gamma$ that contributes most to the spurious output on $\vct{\beta}$. The
intuition is that $\gamma$ should not be merged with any other neuron, as any
over-approximation of the behavior of $\gamma$ has a high chance of
introducing $\vct{\beta}$.

Thus, we do refinement in two steps. Firstly we find the culprit neuron
$\gamma$. Then, we find a cut in the tree that ensures that $\gamma$ is not
merged with any other neuron.

\subsubsection{Finding $\gamma$}

Many possible heuristics may be used to identify the culprit neuron $\gamma$,
and our framework is agnostic to the specific heuristic chosen. In this work, we
use a heuristic based on the 'gradient-guided refinement' described in
\cite{lin-comb-abs-jan}. A neuron is designated as the culprit neuron $\gamma$
when the following value is maximum for that neuron: 

\begin{equation*}
\begin{aligned}
    \|v^{*}_{\gamma}(\vct{\beta}) - v_{\gamma}(\vct{\beta})\|_{2} \cdot 
    \big| \frac{\delta y(\vct{\beta})}{\delta v_{\gamma}} \big|
\end{aligned}
\end{equation*}

Here, $v_{\gamma}(\vct{\beta})$ is the value at the neuron $\gamma$ for input
$\vct{\beta}$ in the original \cnc, while $v^{*}_{\gamma}(\vct{\beta})$ is the
value of the neuron $\gamma$ has been merged to in our current \abs.
$\frac{\delta y(\vct{\beta})}{\delta v_{\gamma}}$ is the gradient of the output
$y$ of \cnc with respect to the value at $\gamma$ for the input $\vct{\beta}$.

\subsubsection{Cutting the Tree}

We wish to find a cut in the tree where $\gamma$ is not merged to any other
neuron, while also making sure that as many neurons remain merged as possible
(therefore minimizing the increase in size of \abs). To do this, we delete
precisely those nodes that are dependent on $\gamma$, starting form the parent
of $\gamma$ and moving up the tree following the parent of each.

Once we have cut the tree and decided on which neurons to leave merged, the
actual merge operation is the exact same as that followed by \cite{cegar-nn}
(Section \ref{s:nn-sam}). Therefore, we are able to provide concrete soundness
guarantees.

In our example, the culprit neuron is $\nr{1}{3}$. Thus, we traverse the tree
following the blue edges Figure \ref{fig:Order_of_merging}, undoing $m_5$ and
$m_6$. This produces three trees, corresponding to leaving $\nr{1}{0}$ and
$\nr{1}{1}$ merged, while undoing the merge of $\nr{1}{2}$ and $\nr{1}{3}$.
Therefore, we get the \abs shown in Figure \ref{fig:tree_cut_refine}. Note
that in contrast to the refinement process followed by \cite{cegar-nn} \todo{Ref
prev sections}, we retain merges of neurons that are semantically close, avoid
proliferation of singletons and achieve a smaller \abs that is sufficient to
prove the property in fewer iterations.

\input{tree_cut_refine.tex}

%\pagebreak
%
%We use the tree produced in the previous Section \ref{s:tree} to guide our
%refinement process. Starting with the 
%entire tree where everything is merged. We leverage it to refine the network.
%This process commences by identifying the ``culprit neuron $\gamma$'' 
%selected for refinement. A ``culprit neuron'' in a merge group is selected 
%on the basis of how much the neuron contributed to the output. If change in 
%output of  neuron changes the value of the output neuron significantly then
%that neuron is a good candidate for ``culprit neuron". 
%
%Following this, we reverse all merges dependent on the culprit 
%neuron $\gamma$. Therefore, refinement essentially involves finding a 
%cut-point in the tree, precisely where all merges dependent on the 
%culprit neuron $\gamma$ are undone. Each cut produces a set of trees, 
%the merge groups then consist of neurons in the leaf nodes of the  these trees.
%Therefore finding new merge groups for refinement is therefore just finding a 
%cuts in the tree.
%
%\sncmt{Do we need to tell the LCA algorithm here?}
%
%Consider Figure \ref{Figure 2}, illustrating the merging sequence of 
%neurons $n_{(1,0,Inc)}$, $n_{(1,1,Inc)}$, $n_{(1,2,Inc)}$, and 
%$n_{(1, 3, Inc)}$. If, for instance, the neuron $n_{(1,3,Inc)}$ 
%is identified as the problematic neuron based on a counter-example, 
%we will reverse all the merges dependent on the $n_{(1,3,Inc)}$ neuron,
% including $\textit{Merge 2}$ and the $\textit{Root Node}$ merge. 
% Consequently, after implementing this reversal indicated in Figure 
% \ref{Figure 2}, our refinement phase will yield three distinct merge groups. 
% The first merge group comprises two neurons, namely $n_{(1,0,Inc)}$ and
% $n_{(1,1, Inc)}$. The second merge group and the third merge have single 
% neurons $n_{(1,2, Inc)}$ and $n_{(1,3,Inc)}$, respectively.
%
% A neuron, denoted as $\gamma$, is designated as a culprit neuron within a
%specific layer when absolute value of the product of the difference between
%$(v_{Rep(\gamma)}$ and $v_{\gamma})$ and the effective weight is maximized.
%\todo{Add the 3 different methods generating counter-examples. Scores should be
%avg over $\beta$ for a particular $\gamma$.}
%
%$||(v_{Rep(\gamma)} - v_{\gamma})||_{2} \cdot |(\textit{effective\_weight})|$
%
%In this context, $Rep$ signifies the representative neuron for neuron $\gamma$,
%$v_{\gamma}$ represents the value of the neuron $\gamma$ at counter-example $\beta$
%and $\textit{effective\_weight}$ represents the how much does the value of output
%neuron changes with respect to change in the value of the neuron under consideration,
%essentially corresponding to the ``$\textit{gradient}$'' at that particular example 
%``$\beta$''.

%\begin{algorithm}
%    \caption{Finding Cuts in the Tree (find\_new\_merge\_groups)}
%    \begin{algorithmic}[1]
%        \State $\gamma= \arg \max_i \|v_{Rep(i)} -v_i \|_2 \cdot | \textit{effective\_weight}| $ 
%        \State Find a sequence of nodes, $t_1,t_2,t_3,..,t_k$ representing a  path from $t_1=$root to $t_k=\gamma$.
%        \State Remove the nodes $t_1,t_2,..,t_{k-1}$ denoting the merges dependent on $\gamma$ through this path, leading to our connected tree being split into a collection of disconnected sub-trees.
%        \State New merge groups are the leaf nodes in our disconnected graph.
%    \end{algorithmic}
%    \hspace*{\algorithmicindent} \textbf{Output} New Merge Groups
%\end{algorithm}


\subsection{Optimality of Tree}

The tree $T$ produced in Section \ref{s:tree} captures an optimal ordering of
merge operations with respect to the semantic information in the following
sense:

Let $T_1$ and $T_2$ be two sub-trees of $T$. Then, the maximum value of $\cls$
for any two neurons that are leaves $T_1$ (or $T_2$) is smaller than the maximum
value of $\cls$ for any two neurons one from $T_1$ and another from $T_2$.
\dmcmt{Make this a lemma?}

This fact can be easily proved via induction on the combined size of $T_1$ and
$T_2$ \todo{Ref appendix}. Thus, we have that for any cut in the tree, the
maximum difference in the semantic behavior for nodes that have been left merged
is less than the maximum difference in semantic behavior for nodes that have
been un-merged.

% \subsection{Optimality of the Trees}
% Our objective is to determine the most efficient order for merging neurons, minimizing the introduction of over-approximation at each step. This approach aims to avoid creating networks with excessive over-approximation, which could lead to the generation of spurious counter-examples in response to queries. Opting not to mitigate over-approximation at each step would result in an increased number of refinement steps. This essentially entails making additional solver calls, incurring significant costs to eliminate the spurious counter-examples.


% Nevertheless, during the initial merging process (until saturation is reached), the root node ``$\rho$'' will exhibit the same level of over-approximation across all conceivable merging scenarios—for all possible tree sequences. Nevertheless, when we descend one level down the tree to explore the children nodes of our original root node $\rho$ for the purpose of identifying a cut for refinement, we discover varying levels of over-approximation manifesting in the root nodes of the resultant sub-trees. These differences are a result of the different merging scenarios pursued to construct those individual trees.

%\begin{algorithm}[H]
%\caption{Cluster Merging Algorithm (find\_abstraction\_tree)}
%\label{Cluster Merging Algorithm}
%\begin{algorithmic}[1]
%    \State Initialize every simulated distance vector as a singleton cluster.
%    \State Initialize $C=\{v_1,v_2,v_3,..\}$ as the set of singleton clusters.
%    \State Initialize a Binary Tree $T$ with leaves as $\{(n_1),(n_2),(n_3),..\}$ corresponding to $\{v_1,v_2,v_3,..\}$.
%    \State Initialize $V$ as a set of visited nodes, empty at first.
%    
%    \Function{MergeFunction}{$u, v$}
%        \If{All nodes are classified as \textbf{Inc}}
%        {
%        
%            \Return $\max(u, v)$
%        }
%        \Else{ }
%        {
%        
%            \Return $\min(u, v)$
%        }
%        \EndIf
%    \EndFunction
%    
%    \While{$|C|>1$}
%        \State $v_j, v_j = \arg\min_{\substack{a, b \in C}} \| a - b \|_2$
%        \State Set $w=\text{MergeFunction}(v_i,v_j)$
%        \State Let nodes from $T$ not in $V$ corresponding to $v_i,v_j$ be $m_i$ and $m_j$
%        \State Remove $v_i,v_j$ from $C$ and add $w$ to $C$.
%        \State Make $(m_i \cup m_j)$ the parent of $(m_i)$ and $(m_j)$ in tree $T$
%        \State Add $m_i$ and $m_j$ to $V$.
%    \EndWhile
%\end{algorithmic}
%\end{algorithm}

% While the optimal tree, representing the optimal merging sequence, can aid in the refinement process by guiding the reversal of merges, finding such an optimal tree poses is extremely challenging. Even when dealing with only `n' Increment (Inc) neurons that have been merged to saturation, the total number of possible trees is given by $(2n-3)!!$, making the task of determining the truly optimal tree from these options extremely challenging.

% Since finding this ideal tree is a challenging task, we employ hierarchical clustering (Algorithm \ref{Cluster Merging Algorithm}) as an approach to approximate and derive such a tree. Initially, we simulate our network using a set of `$k$' inputs. Subsequently, we employ cluster analysis on these `$k$' points to construct a hierarchical arrangement of clusters. This process initiates with data points corresponding to simulated values (observation values in the observation vector) of a neuron  forming their own cluster. The clusters are then systematically combined based on their similarity, thereby generating a hierarchy of clusters. The choice of similarity measure is the ``$distance \hspace{1mm} metric$" between clusters. We have used ``$Euclidean \hspace{1mm} Distance$" as our distance metric. Given that the data points to perform this hierarchical clustering originate from the values of the simulated neurons, this hierarchical clustering effectively reflects the methodology we employ to merge the neurons.

% For example, in Figure \ref{Figure: Order Of Merging}, we conducted a simulation of our network on three data points. Subsequently, we examined the observation vectors corresponding to these points. Utilizing the hierarchical clustering algorithm, the initial selection for merging  will involve $(n_0^{1}, Inc)$ and $(n_1^{1}, Inc)$ because of the fact that their Euclidean distance is minimum. This forms $\textit{Merge 1}$ in Figure \ref{Figure: Order Of Merging}. The observation vector for $\textit{Merge 1}$ ($\nu_\textit{Merge1 }$) is the max of the $\nu((n_0^{1}, Inc))$ and $\nu((n_1^{1},Inc))$ which is \{1.5, 3, 4.5\}. For decrement nodes the observation vector would be minimum of the observation vector of the corresponding decrement nodes. The next merging step involved selecting $\nu((n_2^{1}, Inc))$ and $\nu((n_3^{1}, Inc))$ and merging these two neurons, representing $\textit{Merge 2}$ in Figure \ref{Figure: Order Of Merging}. The observation vector for $\textit{Merge 2}$ is now \{4, 8, 12\}. Ultimately, the Merge1 merge group is merged with the $\textit{Merge 2}$ merge group to create the Root Node in our network.

%\begin{figure}[H]
%    \centering
%    \includegraphics[width = 0.5\textwidth]{diagrams/good_vs_bad_merges.pdf}
%    \caption{Ways of Merging}
%    \label{Figure 3}
%\end{figure}

% This approach of merging neurons based on similarity proves advantageous as it helps in reducing number of refinement steps. For instance, consider the task of checking whether $\forall v_{0}^{0} \in [0, 1]$ implies $v_{0}^{2} < 10$. If we had the neuron $(n_{3}^{1}, Inc)$ as the culprit neuron and then if we follow the second merging approach depicted in Figure \ref{Figure 3}, then we would have been compelled to reverse both $\textit{Merge 1}$ and $\textit{Merge 2}$. However, by employing the first merging approach, undoing only the $\textit{Merge 2}$ becomes sufficient, resulting in a reduction in the number of refinement steps required.


\subsection{Overall Algorithm}
\begin{algorithm}[H]
    \caption{Overall Algorithm}
    \label{Overall Algorithm}
    \begin{algorithmic}[1]
        \State $\mathcal{N'}$ = split\_Inc\_Dec($\mathcal{N}$)
        \State $\mathcal{N''}$ = abstract\_network($\mathcal{N'}$)
        \State simulation\_dict = simulate\_network($\mathcal{N'}$)
        \State $\mathcal{T}$ = find\_abstraction\_tree($\mathcal{N'}$, $simulation\_dict$)
        \If{verify($\mathcal{N''}$, $\kappa$, $\lambda$) is UNSAT}{

            \Return Property Holds
            }
        \Else
            \State Extract counter-example $\beta$
            \If{$\beta$ is not a spurious counter-example}
            {

                \Return ($\beta$, Property Violated)
            }
            \Else
                \State Find culprit neuron $\gamma$
                \State $merge\_groups$ = find\_new\_merge\_groups($\mathcal{T, \gamma}$)
                \State $\mathcal{N''} = get\_abstract\_network(merge\_groups)$
                \State \textbf{goto} step 5
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}
