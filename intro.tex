
\section{Introduction}
\dmcmt{There may be repetation with abstract, but that is fine right?}

Advances in Deep Neural Networks (\dnn) have enabled the scalable solution of
several previously intractable problems such as image recognition and natural
language processing. Due to this, \dnn have increasingly assumed a central role
across various domains. These include several safety-critical domains like
healthcare \cite{b1}, where they contribute significantly to medical diagnosis
and predictive analysis \cite{b2}, and autonomous vehicles, where DNNs serve as
the backbone for sophisticated perception systems, supporting tasks such as
object recognition and decision making \cite{b3}. 

However, given the enormous size and the substantial resource requirements of these \dnn in terms of CPU, memory and power, their implementation on embedded devices and real-time systems is often infeasible. This issue becomes especially apparent when integrating DNNs into embedded and real-time systems for performing safety-critical tasks, such as obstacle recognition in autonomous vehicles. Furthermore, \dnn are well known to be vulnerable to adversarial~\cite{l-bfgs,fgsm,deep-fool,pgd,ground-truth-adv-attack,cw-attack}
and backdoor
attacks~\cite{backdoor-poisoning}, and are also difficult to interpret.
While a number of formal analysis techniques have been proposed to build
trust on the reliability of \dnn in safety-critical settings, including
verification \cite{reluplex,deeppoly,crown,beta-crown,cegar-nn} Â and formal
explainability \cite{overview-fxai,minimal-image-fxai}, the size of the DNNs
\sncmt{Say here: and the NP-Hardness of solving queries involving DNNs}
continues to be the limiting factor for the scalability of these techniques.
To tackle both these issues, it is imperative to reduce the size of the \dnn
involved while maintaining a strong formal connection to the original network, and
preserving desirable safety properties.

%However, given the enormous size and the substantial resource requirements of
%these DNN solutions in terms of CPU, memory and power, their implementation
%on embedded devices is often infeasible.  This issue
%becomes especially apparent when integrating DNNs into embedded systems for
%performing safety-critical tasks, such as obstacle recognition in autonomous
%vehicles. 
%Additionally, \dnn are well known to be vulnerable to
%adversarial~\cite{l-bfgs,fgsm,deep-fool,pgd,ground-truth-adv-attack,cw-attack}
%and backdoor
%attacks~\cite{backdoor-poisoning}, and are also difficult to interpret.
%Therefore, to build trust on safety and business-critical embedded systems
%that utilize \dnn, it is critical to understand, interpret and validate their
%behaviour via formal analysis
%\cite{overview-fxai,minimal-image-fxai,backdoor-verification,nn-lander-verif,camera-verif-dsouza,generalization-verif}.
%While a number of techniques have been proposed to build
%trust on the reliability of \dnn in safety-critical settings, including
%verification \cite{reluplex,deeppoly,crown,beta-crown,cegar-nn}  and formal
%explainability \cite{overview-fxai,minimal-image-fxai}, the size of the DNNs
%continues to be the limiting factor for the scalability of these techniques.
%Therefore, for these two reasons, the applicability of \dnn in safety critical
% embedded systems remains limited.

A typical approach within formal methods to reduce the complexity of any object
while maintaining a strong formal connection with the original network is
\emph{abstraction}.
For \dnn, structural abstraction based on the \emph{syntax} (the local weights
and biases at each neuron of the DNN) forms the basis of several techniques
\cite{cegar-nn,cegarette,cleverest-nn,conv-abs-gk} 
that work by converting a large \emph{concrete}
DNN \cnc into a smaller \emph{abstract} DNN \abs via \emph{merging} groups
of neurons in \cnc into single neurons in \abs. 
Each such merge is done in a
way that ensures that there are \textit{concrete}, formal soundness guarantees
linking the behavior of \cnc and \abs, thus maintaining safety-critical
properties. 
In particular, one can verify properties on \abs, and
using the concrete soundness guarantees, lift the result to \cnc and argue
about its reliability. 
However, while these techniques have been shown to extend the scalability of
neural network verification techniques, they do not take the \emph{semantic}
behavior of the network into account, thereby producing sub-optimally large
\abs. \cite{cegar-nn}. 
In fact, in \cite{cegar-nn}, the \abs produced was sometimes observed to be
larger than the original \cnc.
This sub-optimality with respect to size prevents these techniques from being
useful for compressing \dnn for deployment in safety-critical
resource-constrained environments.

On the other hand, neural network compression techniques \cite{dnn-compression}
and semantic abstraction techniques \cite{deep-abstract,lin-comb-abs-jan} take
into account the global \textit{semantic} behavior of the network, and are able
to achieve a significant reduction in size.
However, heuristic based compression techniques \cite{dnn-compression} do not
formally maintain any connection with \cnc, while semantic abstraction
\cite{deep-abstract,lin-comb-abs-jan} techniques only provide some limited
kinds of formal connections with \cnc. 
In particular, the guarantees provided by clustering based methods like
\cite{deep-abstract} are limited to lifting specific bound propagation based
proofs from \abs to \cnc, 
and those provided by linear combination based methods like
\cite{lin-comb-abs-jan} only bound the difference in behavior of \abs and \cnc
on a finite subset of the input space.
Thus, without strong formal guarantees connecting the behaviors of \abs and
\cnc, the \abs may not preserve desired safety properties.

%Structural abstraction based on the \textit{syntax} (the local weights and
%biases at each neuron of the DNN) forms the basis of several techniques
%attempting to alleviate this issue
%\cite{cegar-nn,cegarette,cleverest-nn,conv-abs-gk}. 
%These techniques work by converting a large \textit{concrete}
%DNN \cnc into a smaller \textit{abstract} DNN \abs via \textit{merging} groups
%of neurons in \cnc into single neurons in \abs. 
%Each such merge is done in a
%way that ensures that there are \textit{concrete}, formal soundness guarantees
%linking the behavior of \cnc and \abs. 
%Then, one can make queries on \abs, and
%using the concrete soundness guarantees, lift the result to \cnc and argue
%about its reliability. 
%However, these techniques do not take into account the
%global \textit{semantic} behavior of the network, thus produce potentially
%suboptimal abstractions.

%On the other hand, neural network compression techniques \cite{dnn-compression}
%and semantic abstraction techniques \cite{deep-abstract,lin-comb-abs-jan} take
%into account the global \textit{semantic} information within the network.
%However, compression techniques provide no soundness guarantees, while 
%semantic abstraction techniques provide limited soundness guarantees. In
%particular, the guarantees provided by clustering based methods like
%\cite{deep-abstract} are limited to
%lifting specific bound propagation based proofs, and those provided by
%linear combination based methods like \cite{lin-comb-abs-jan} only characterize
%soundness on a finite subset of the
%input space. This limits the applicability of these techniques to situations
%where concrete, hard guarantees may be necessary.

\dmcmt{In this section, we end up saying "setting up a cegar loop thrice"}

In this work we combine the syntactic and semantic approaches into a single
framework for generating an abstract network. By splitting and labeling the
neurons \textit{inc} and \textit{dec}, similar to \cite{cegar-nn}, and
restricting ourselves to merges involving only similarly labeled neurons, we
provide a concrete formal link the behavior of \cnc and \abs via
syntactic constraints. At the same time, to take into account the global
semantic behavior, we introduce a semantic closeness metric between two neurons
in the same layer. Using this metric, we construct a tree of merge operations
that captures the relative contribution of each merge to the quality of the
abstraction. We propose a refinement procedure \dmcmt{here implicitly we say we
are setting up cegar loop} that uses
this tree as a guide
to undo the merge operations that contribute most to the poor quality of \abs.
We show that in the resulting refined network, groups of neurons that remain
merged are semantically closer than neurons that get un-merged. Thus, we are
able to refine \abs in a way that is optimal with respect to the semantic
behavior. Thus, combining both syntactic and semantic approaches, our framework
is able to find an \abs that preserves desired safety properties and is of a
smaller size.

We assemble these pieces into an abstraction-refinement framework \dmcmt{ here
again we say set up cegar loop} that can
generate a small size \abs by starting with a fully merged network and
iteratively refining until a strong enough network is obtained. To demonstrate
the usefulness of this framework we set up a
\cegar loop\dmcmt{ here again we say set up cegar loop}  
to verify the \acasxu networks. In these experiments we find that we
are able to produce smaller networks than existing works that are still strong
enough to verify the property.

\dmcmt{Alternate para, without the repeats::}

In this work we combine the syntactic and semantic approaches into a single
framework for generating an abstract network. By splitting and labeling the
neurons \textit{inc} and \textit{dec}, similar to \cite{cegar-nn}, and
restricting ourselves to merges involving only similarly labeled neurons, we
provide a concrete formal link the behavior of \cnc and \abs via
syntactic constraints. At the same time, to take into account the global
semantic behavior, we introduce a semantic closeness metric between two neurons
in the same layer. Using this metric, we construct a tree of merge operations
that captures the relative contribution of each merge to the quality of the
abstraction. We propose a CEGAR framework where
the refinement procedure
uses this tree as a guide
to undo the merge operations that contribute most to the poor quality of \abs.
In the resulting refined network, groups of neurons that remain
merged are semantically closer than neurons that get un-merged, 
Thus, combining both syntactic and semantic approaches, our framework
is able to find an \abs that preserves desired safety properties and is of a
smaller size.

To demonstrate
the usefulness of this framework we set up 
experiments
verifying the \acasxu networks. In these experiments we find that we
are able to produce smaller networks than existing works that are still strong
enough to verify the property.

