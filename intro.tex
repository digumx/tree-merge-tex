
\section{Introduction}

Advances in Deep Neural Networks (\dnn) have enabled the scalable solution
of several previously intractable problems like image recognition.\todo{ add
more egs, cite} Due to this, \dnn have been increasingly assumed a central role
across various domains. These include several safety critical domains like
healthcare \cite{b1}, where they contribute significantly to medical diagnostics
and predictive analysis \cite{b2}, and autonomous vehicles, where DNNs serve as
the backbone for sophisticated perception systems, supporting tasks such as
object recognition and decision-making \cite{b3}. 

However, \dnn are well known to be vulnerable to adversarial attacks \todo{
cite} and, being generally un-interpretable, can often produce unexpected
behaviours. Therefore, to build trust on safety critical systems that utilize
\dnn, it is \dmcmt{has become?} critical to understand, interpret and validate
the possible behaviors of these \dnn via formal analysis. \dmcmt{ Cite some dnn
verif case studies, like nn-lander-verif, Prof Deepak's work, etc? Mention
verification / interpretability here?}

A number of techniques have been proposed to build trust on the reliability
of \dnn in safety-critical settings, including verification \cite{reluplex,
deeppoly} \todo{cite others}, formal explainability \cite{overview-fxai,
minimal-image-fxai} and others \todo{cite}. For most of these
techniques, it is necessary to make queries to neural network verification
solvers (such as \cite{reluplex}) to ascertain the trustworthy-ness of the \dnn.
\dmcmt{What I mean here is to get verif / produce the interpretation. Is wording
okay?} However, since solving these queries is NP-Hard (see appendix of
\cite{reluplex}) \dmcmt{Should I point to appendix?}, formal analysis of \dnn
often faces scalability issues, and is highly sensitive to the size of the \dnn.

Structural abstraction based on the \textit{syntax} (the local weights and biases at each
node of the \dnn) form the basis of several techniques attempting to alleviate
this issue \cite{cegar-nn, cegarette, cleverest-nn, conv-abs-gk}. These
techniques work by converting a large \textit{concrete} \dnn \cnc into a smaller
\textit{abstract} DNN \abs via \textit{merging} groups of neurons belonging to
the same \textit{merge group} in \cnc into single neurons in \abs. Each such
merge step is done in a way that ensures that there are strong, formal
\textit{soundness} guarantees linking the behavior of \cnc and \abs. Then, one
can make queries on \abs, and using the strong \textit{soundness} guarantees,
lift the results to \cnc argue about it's reliability. However, these techniques
do not take into account the global \textit{semantic} behavior of the network,
thus producing potentially sub-optimal abstractions potentially sub-optimal
abstractions. \dmcmt{This last bit is not necessarily exposed by our
experiments..}

On the other hand, neural network compression techniques \todo{cite} and
semantic abstraction techniques \cite{deep-abstract, lin-comb-abs-jan} take into
account the global semantic information within the network. However,
compression techniques provide no soundness guarantees, while the semantic
abstraction techniques provide limited soundness guarantees. In particular, the
guarantees provided by \cite{deep-abstract} are limited to lifting specific
bound propagation based proofs, and those provided by \cite{lin-comb-abs-jan}
only characterise soundness on a finite subset of the input space.\dmcmt{Is this
okay, or too much detail / bad in any other way?} This limits the applicability
of these techniques to situation where stronger, hard guarantees may be
necessary.

In this work we combine the syntactic and semantic approaches into a single
framework for generating an abstract network. By splitting and labelling the
neurons \textit{inc} and \textit{dec} similar to \cite{cegar-nn}, we can
restrict ourselves to merges involving only similarly labelled neurons, and can
guarantee soundness. Using the global semantic information obtained from
simulating \cnc on a set of input points, we estimate the IO-similarity of
neurons \dmcmt{Should we say this: similar to \cite{deep-abstract}?}. Based on this
similarity, we then construct a partial order of possible merges in each layer
capturing the relative contribution of each merge to the quality of the
abstraction. Finally, if the resulting \abs
is not strong enough to be effective \dmcmt{Avoiding language like
over-approximate}, we can refine it by eliminating the merge operation involving
a neuron that contributes most to the poor quality \dmcmt{Wording here? Or use
weakness?} of \abs and any merge operations that appear higher in the partial
order.

We describe a way to efficiently implement this partial order based refinement
procedure to be able to iterate through potential refinements with minimal
computational overheads. We do this by creating tree-based data structures that
allows us to pre-compute a significant portion of the relevant information at
the beginning of the refinement loop. Then, refinement operations can be
performed via very quick index manipulations. This allows us to iterate through
and screen several possible refinements very fast. \dmcmt{We don't explicitly
measure the speed in the experiments section, so is this okay?}

We demonstrate the effectiveness of our abstraction via three case studies.
Firstly, we attempt to verify the standard \acasxu benchmarks via a CEGAR loop
utilizing our abstraction and refinement procedures. \dmcmt{Not saying anything
more} Then, we plot the number of spurious counterexamples our technique
introduces with respect to the size reduction obtained for $\epsilon$-robustness
properties on the \mnist networks, demonstrating significant size reductions
with minimal introduction of spurious counterexamples. 

There are several classification problems for neural networks where
\textit{false negatives} with respect to a certain class is highly undesirable,
for example,
medical diagnosis and collision detection. In the final case study, we
demonstrate how our method can be used to obtain compressed networks for these
applications with the guarantee that the compression procedure will not
introduce any new false negatives. We plot the number of \textit{false
positives} introduced in this compression process as well, and find that our
method is able to produce compressed networks without introducing any new false
negatives and very few false positives. \dmcmt{Is one para for this okay?}
