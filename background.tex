\section{Background}
\subsection{Notation}

We restrict ourselves to fully-connected, feed-forward neural networks with
\relu activation function. 
Neurons in our network are denoted as $\nr{i}{l}$, where $i$ signifies the 
neuron number in layer $l$.  % Used
The function taking a given input $\vct{x}$ to the value of $\nr{i}{l}$ is
represented by $\nrf{i}{l}(\vct{x})$, % Used 
The function $\ob{i}{l}$ takes a list of input vectors
$[\vct{x_1} {\cdots} \vct{x_N}]$ and returns a vector with
$[\nrf{i}{l}(\vct{x_1}) {\cdots} \nrf{i}{l}(\vct{x_N})]$ for a particular
neuron $\nr{i}{l}$.  %Used

\subsection{Formal Analysis of Neural Networks }
\label{s:form-an}

Several techniques and methods have been studied to improve the reliability and
trustworthiness of \dnn deployed in safety-critical settings via formal
analysis. This includes verifying \dnn with respect to a given
safety property
\cite{reluplex,cegar-nn,deeppoly,cegarette,cleverest-nn,conv-abs-gk,deep-abstract,lin-comb-abs-jan},
providing formal explanations of
the behavior of \dnn \cite{minimal-image-fxai,overview-fxai}, and defending
against backdoor attacks \cite{backdoor-verification}.
 To provide the formal guarantees behind the
analyses performed, all of these techniques rely on making \textit{neural
network queries}. 

These neural network queries are of the form $(P, \mcnc, Q)$ and ask: if
for all inputs $\vct{x}$ to $\mcnc$ for which the formula $P$ holds, 
the formula $Q$ also holds for the output $\mcnc(\vct{x})$. While there are
several
tools that can handle such queries, like \marabou, \abcrown and \neuralsat,
scalability remains an issue, and so reducing the size of \cnc is desirable.

\subsection{Semantic Compressions and Abstractions with Empirical Guarantees}
\label{s:emp-abs}

Several techniques utilize semantic information, typically extracted via
simulation of the \dnn, to obtain a smaller \abs. 
Neural network compression techniques \cite{dnn-compression}, produce small
\abs, but the
behavior of \abs in connection to \cnc is only characterized empirically.
Similarly, some semantic abstraction techniques \cite{lin-comb-abs-jan} provide
bi-simulation guarantees bounding the difference in the behavior of \abs and
\cnc on a finite set of input points, typically a subset of the training
dataset. Since these techniques characterize the behavior of \abs only on a
finite set of
input points, the trust obtained on the connection between \cnc and \abs is only
of an empirical nature.
Other techniques, like \cite{deep-abstract}, use bi-simulation to lift
interval bound propagation performed on \abs to get sound bounds on \cnc. While
this does provide a sound proof, interval bounds are typically not strong enough
to prove many interesting and practically relevant neural network queries.

\subsection{Strong Formal Connections between \cnc and \abs}
\label{s:conc-abs}

Structural abstraction techniques, including \cite{cegar-nn}, provide the
following strong formal connection between the behavior of \cnc and \abs:
$\forall x, \mabs(x) \geq \mcnc(x)$. 
Any general neural network query can be converted to a query of
the form \linebreak $(P, \mcnc, y < c)$ for some $c$
by encoding the postcondition as extra layers in \cnc 
\cite{cegar-nn,reluplex,lipschitz-reach}.
Therefore, this notion of formal connection allows one to abstract the larger
\cnc to a smaller \abs, dispatch the
easier query $(P, \mabs, y < c)$ using a solver call, and argue that the
original query holds \cite{cegar-nn,cegarette,cleverest-nn}. 
This immediately makes such an \abs useful
for accelerating several formal analysis techniques (Section \ref{s:form-an}).
Therefore, in this work, we focus on developing a framework that produces
abstract networks that maintain this formal connection.

\subsection{Syntactic Neural Network Splitting and Merging}
\label{s:nn-sam}

In order to ensure soundness guarantees when transforming \cnc into \abs, 
we adopt a modification of the four-way split approach proposed in
\cite{cegar-nn}.
In this approach, a two-way split is used instead
\cite{chauhan2022efficiently,liu2022abstraction,10.1145/3644387}, 
partitioning neurons in \cnc into duplicate
copies labeled with either {\inc or \dec}. 
This partitioning is done in a way that 
ensures that any increase 
(decrease) in the value of a neuron labeled \inc (\dec) only results in 
an increase in the output value. This can be done because it is not
necessary to perform \posc-\negc splitting to soundly obtain an \inc-\dec
split, as seen in
\cite{chauhan2022efficiently,liu2022abstraction,10.1145/3644387}. Then,
following \cite{cegar-nn}, a sound 
abstraction can be obtained by \textit{merging} all similarly 
labeled neurons as follows: if the neurons have
the label \inc (\dec), replace incoming edges from the same
previous layer neuron with a single edge with the maximum (minimum) of the
original edge weights. Outgoing edges to the same next layer neuron are replaced
with a single edge with the sum of the weights for both the \inc and \dec cases.

\input{original_net.tex}
\input{gk_abs_sat.tex}

For example, consider the network and property in Figure
\ref{fig:Original_Net_Property}. For this example, all the neurons in the output
and middle layer get classified as \inc. Thus, they are all merged together,
leading to the network in Figure \ref{fig:full_abstract_net}.

Note that this process only considers the syntactic structure of the network, no
semantic information is used.

\subsection{ Syntactic Refinement }

The fully merged network obtained in Section \ref{s:nn-sam} may not be
sufficiently strong to be able to dispatch, that is, there may be 
spurious counterexamples. 
In such situations, a common approach to obtaining a better-quality \abs is to
perform refinement steps based on a \gencex $\vct{\beta}$
\cite{cegar-nn,cegarette,cleverest-nn}. 
In existing techniques, this is typically done by restoring a single neuron
coming from \cnc that had been merged with other neurons in \abs. The neuron
chosen is typically one whose contribution to $\vct{\beta}$ is estimated to be
the highest.

These techniques, however, do not consider any semantic behavior to guide their
refinement. As such, the refinement process tends to produce a large number of
restored neurons that are not merged with any other neurons. A proliferation of
these \emph{singleton} neurons lead to \abs having a larger size. At the same
time, a large group of neurons remains merged in \abs, which affects the quality
of \abs. 

We can see this in our example. Say the fully merged network in Figure
\ref{fig:full_abstract_net} we get a $\vct{\beta}$ given by the values in \textbf{bold}.
Then, in the next refinement step, the neuron $\nr{3}{1}$ gets restored, giving
us the network in Figure \ref{fig:refine_step_1}. Again, the $\vct{\beta}$
obtained is shown in \textbf{bold}, and the next refinement step restores $\nr{0}{1}$
leading to Figure \ref{fig:refine_step_2}. We see that in the resultant network,
two semantically dissimilar neurons, $\nr{1}{1}$ and $\nr{2}{1}$, remain merged,
while the merges between the similar pairs of neurons $\nr{3}{1}$, $\nr{2}{1}$
and $\nr{1}{1}$, $\nr{0}{1}$ have been un-done. Indeed, the network in Figure
\ref{fig:refine_step_2} still admits spurious counterexamples, as seen by the
values in \textbf{bold}, and we end up refining all the way to the original
network.

\input{gk_refine1.tex}
\input{gk_refine2.tex}
